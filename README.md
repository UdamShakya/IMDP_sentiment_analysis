# ğŸ¬ IMDB Sentiment Analysis (NLP Project)

## Overview
This project applies **Natural Language Processing (NLP)** techniques to classify IMDB movie reviews as **positive** or **negative**.  
It starts with a **baseline LSTM model** and gradually improves using **GRU, CNN, and Transformer-based models (BERT)**.  
Future steps include deployment via **Streamlit/Gradio** for interactive demos.

## Features
- Data loading & preprocessing
- Baseline LSTM model
- Evaluation metrics & visualizations
- Advanced models (GRU, CNN, Transformers)
- Easy deployment for demo

# IMDB Sentiment Analysis â€“ NLP Model ğŸš€  

This project builds and improves a sentiment analysis model for IMDB reviews using deep learning.  
Each day introduces structured improvements â€” like a research log.  

---

## ğŸ“… Day 1 â€“ Baseline Sentiment Analysis Model

**Objective:** Build the simplest possible sentiment classifier for IMDB reviews.  

### âœ… Steps Implemented
1. **Data loading (`data_loader.py`)**
   - Downloaded IMDB dataset (25k training, 25k testing).  
   - Reviews already tokenized into integers.  
2. **Preprocessing (`preprocess.py`)**
   - Padded/truncated reviews to fixed length (200 tokens).  
3. **Baseline model (`train.py`)**
   - Architecture: `Embedding â†’ GlobalAveragePooling â†’ Dense â†’ Sigmoid`.  
   - Fast but ignores word order.  
4. **Evaluation (`evaluate.py`)**
   - Tested on validation and test data.  
   - Training curves + bar chart (correct vs incorrect predictions).  

### ğŸ“Š Results (Baseline Model)
- Test Accuracy: ~84%  
- Training was quick but model failed on complex sentences (since word order is ignored).  

ğŸ“¸ *Placeholder for screenshot of loss/accuracy curve:*  
![Baseline Training Curve](images/day1_training_curve.png)  

## ğŸ“… Day 2 Progress

- âœ… Added LSTM-based sentiment analysis model (`train_lstm.py`)
- âœ… Added GRU-based sentiment analysis model (`train_gru.py`)
- âœ… Implemented comparison script for LSTM vs GRU (`compare_models.py`)
- âœ… Added early stopping and model checkpointing (`train_lstm.py`)
- âœ… Enhanced evaluation with confusion matrix and classification report (`evaluate.py`)
- âœ… Updated README with results and explanations

### Example Results:
- **LSTM Accuracy:** ~86%
- **GRU Accuracy:** ~85%
- Confusion Matrix + Classification Report available in `/results`

### Next Steps (Day 3):
- Hyperparameter tuning (embedding dim, hidden units, batch size)
- Add word embeddings visualization
- Try bidirectional LSTM

## ğŸ“… Day 3 â€“ Regularization, Monitoring & Embeddings

**Objective:** Improve generalization, add monitoring, and visualize embeddings.  

### âœ… Steps Implemented
1. **Dropout in LSTM (`train_lstm.py`)**
   - Added `Dropout(0.5)` between stacked LSTMs.  
   - Prevents overfitting by randomly disabling neurons.  
2. **Early Stopping + Model Checkpointing**
   - Stops training if validation loss doesnâ€™t improve.  
   - Saves best model weights.  
3. **TensorBoard Logging (`logs/fit/`)**
   - Added TensorBoard callback.  
   - Run locally with:kkkk

     ```bash
     tensorboard --logdir=logs/fit
     ```
     Open [http://localhost:6006](http://localhost:6006) to view.  
   - TensorBoard shows:
     - Training/validation loss & accuracy  
     - Model graph  
     - Weight/activation histograms  
     - Compare multiple experiments side by side  
4. **Embedding Visualization (`visualize_embeddings.py`)**
   - Extracted embeddings â†’ reduced with t-SNE â†’ plotted 2D map.  
   - Shows semantic clustering of words.  

### ğŸ“Š Results (Day 3 Enhancements)
- Dropout stabilized validation accuracy.  
- TensorBoard allowed **experiment comparison**.  
- Embedding visualization showed similar words clustering together.  

ğŸ“¸ *Placeholder for screenshots:*  
- ![TensorBoard Accuracy Curve](images/day3_tb_accuracy.png)  
- ![TensorBoard Loss Curve](images/day3_tb_loss.png)  
- ![Word Embedding Visualization](images/day3_embeddings.png)  

---

## ğŸ§­ Project Timeline
- **Day 1:** Simple baseline â†’ proof-of-concept  
- **Day 2:** Advanced sequence models (LSTM, GRU, BiLSTM)  
- **Day 3:** Overfitting control + TensorBoard + embeddings  

---

## ğŸ“… Day 4: Advanced Model Training & Hyperparameter Tuning

On **Day 4**, we focused on exploring **different hyperparameters** (LSTM units, dropout rates, learning rates, batch sizes) to evaluate their effect on IMDB sentiment classification performance.

### ğŸ”¹ What We Did
- Trained **16 different models** with varying hyperparameters.
- Logged training/validation accuracy & loss with **TensorBoard**.
- Applied **EarlyStopping** to prevent overfitting.
- Saved the **best-performing model** automatically.
- Extracted all experiment results into a **CSV file**.
- Visualized outcomes with **training curves** and a **confusion matrix**.

---

### ğŸ“Š Hyperparameter Results
We saved the final validation accuracy/loss for each experiment into a CSV:

ğŸ“„ [Download CSV](images/day4/hyperparam_results_from_logs.csv)

| lstm_units | dropout | learning_rate | batch_size | val_accuracy | val_loss |
|------------|---------|---------------|------------|--------------|----------|
| 64         | 0.2     | 0.001         | 32         | 0.885        | 0.365    |
| 128        | 0.3     | 0.001         | 64         | 0.892        | 0.342    |
| ...        | ...     | ...           | ...        | ...          | ...      |

*(table truncated for readability â€“ see CSV for full results)*

---

### ğŸ“· Training Curves (TensorBoard & Saved Images)

- **TensorBoard Accuracy Screenshot**  
  ![TensorBoard Accuracy](images/day4/accuracy.png)

- **TensorBoard Loss Screenshot**  
  ![TensorBoard Loss](images/day4/loss.png)

- **Generated Accuracy vs Validation Plot**  
  ![Generated Accuracy](images/day4/gen_accuracy.png)

- **Generated Loss Plot**  
  ![Generated Loss](images/day4/gen_loss.png)

---

### ğŸ” Confusion Matrix (Best Model on Test Set)

We also visualized the predictions of the **best model** against the test set:

- **Confusion Matrix**  
  ![Confusion Matrix](images/day4/confusion_matrix.png)

This matrix shows how many positive/negative reviews were classified correctly vs misclassified.

---

### ğŸš€ Key Takeaways
- Models with **128 LSTM units, 0.3 dropout, and learning rate 0.001** achieved the best performance.
- Overfitting was reduced significantly with **dropout + early stopping**.
- TensorBoard allowed us to compare **all 16 experiments visually**.
- CSV + plots make it easier to compare experiments outside of TensorBoard.

---
## ğŸ“Š Day 05 â€“ CNN vs LSTM Comparison

We trained **two deep learning models** on the IMDB dataset:

- **CNN (Convolutional Neural Network)** â€“ captures local n-gram features.
- **LSTM (Long Short-Term Memory)** â€“ captures long-range dependencies in text.

### ğŸ”¹ Results
- CNN achieved ~XX% accuracy.
- LSTM achieved ~YY% accuracy.

### ğŸ”¹ Visualizations
**Training Curves:**
![CNN Plot](results/CNN_plot.png)  
![LSTM Plot](results/LSTM_plot.png)

**Confusion Matrices:**
![CNN Confusion Matrix](results/CNN_confusion_matrix.png)  
![LSTM Confusion Matrix](results/LSTM_confusion_matrix.png)

## ğŸ“… **Day 06 â€” Building the Streamlit App**

### ğŸ§  Overview
Today focused on making our trained NLP model *interactive and accessible* using **Streamlit** â€” an open-source framework for turning machine-learning models into shareable web apps.  
The goal: allow users to enter custom movie reviews and instantly see predictions from both **CNN** and **LSTM** models.

---

### âš™ï¸ Tasks Completed
âœ… Designed a Streamlit interface for real-time IMDB sentiment prediction  
âœ… Added an option to choose between CNN and LSTM models  
âœ… Displayed sentiment predictions with visual cues (ğŸ˜Š / ğŸ˜)  
âœ… Shown model accuracy and evaluation results  
âœ… Ensured compatibility with local virtual environments  
âœ… Documented full setup for deployment

---

### ğŸ§© Key Code Component â€“ `app.py`
```python
import streamlit as st
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
from preprocess import preprocess_data

# Load tokenizer and models
MAXLEN = 200
cnn_model = tf.keras.models.load_model("models/cnn_model.h5")
lstm_model = tf.keras.models.load_model("models/lstm_model.h5")

st.title("ğŸ¬ IMDB Sentiment Analyzer")
st.write("Interactively predict movie review sentiments using trained models.")

review = st.text_area("Enter your movie review here:")

model_choice = st.selectbox("Choose Model", ["CNN", "LSTM"])

if st.button("Predict Sentiment"):
    from tensorflow.keras.datasets import imdb
    word_index = imdb.get_word_index()
    words = review.lower().split()
    encoded = [word_index.get(w, 2) for w in words]  # 2 = unknown
    padded = pad_sequences([encoded], maxlen=MAXLEN)

    model = cnn_model if model_choice == "CNN" else lstm_model
    prediction = model.predict(padded)[0][0]

    sentiment = "ğŸ˜Š Positive" if prediction > 0.5 else "ğŸ˜ Negative"
    st.subheader(f"Predicted Sentiment: {sentiment}")
    st.write(f"Confidence: {prediction:.2f}")
